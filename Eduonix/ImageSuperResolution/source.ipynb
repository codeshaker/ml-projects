{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "\n",
    "# python magic functions\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for peak signal to noise ratio (PSNR)\n",
    "def psnr(target, ref):\n",
    "    \n",
    "    # assume a RGB/BGR Image \n",
    "    target_data = target.astype(float)\n",
    "    ref_data = ref.astype(float)\n",
    "    \n",
    "    diff = ref_data - target_data\n",
    "    diff = diff.flatten('C')\n",
    "    \n",
    "    rmse = math.sqrt(np.mean(diff ** 2.))\n",
    "    \n",
    "    # formula\n",
    "    return 20 * math.log10(255. / rmse)\n",
    "\n",
    "# Define function for mean squared error(MSE)\n",
    "\n",
    "def mse(target, ref):\n",
    "    \n",
    "    # assume a RGB/BGR Image \n",
    "    err = np.sum((target.astype(float)- ref.astype(float)) ** 2)\n",
    "    err /= float(target.shape[0] * target.shape[1])\n",
    "    \n",
    "    return err\n",
    "\n",
    "# define a function that combines all 3 image quality metrics\n",
    "def compare_images(target, ref):\n",
    "    scores = []\n",
    "    scores.append(psnr(target,ref))\n",
    "    scores.append(mse(target,ref))\n",
    "    # Structural similarity index\n",
    "    scores.append(ssim(target,ref, multichannel = True))\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare degraded images by introducing quality distortion via resizing\n",
    "\n",
    "def prepare_images(path, factor):\n",
    "    \n",
    "    # Loop through the files in directory\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        # Open the file\n",
    "        img = cv2.imread(path + '/' + file)\n",
    "        \n",
    "        # find the old and new image dimensions\n",
    "        h, w, c = img.shape\n",
    "        new_height = int(h / factor)\n",
    "        new_width = int(w / factor)\n",
    "        \n",
    "        # resize images - down\n",
    "        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # resize image - up\n",
    "        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n",
    "        \n",
    "        # save the image\n",
    "        print(\"Saving {}\".format(file))\n",
    "        cv2.imwrite(\"images/{}\".format(file),img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving baboon.bmp\n",
      "Saving baby_GT.bmp\n",
      "Saving barbara.bmp\n",
      "Saving bird_GT.bmp\n",
      "Saving butterfly_GT.bmp\n",
      "Saving coastguard.bmp\n",
      "Saving comic.bmp\n",
      "Saving face.bmp\n",
      "Saving flowers.bmp\n",
      "Saving foreman.bmp\n",
      "Saving head_GT.bmp\n",
      "Saving lenna.bmp\n",
      "Saving monarch.bmp\n",
      "Saving pepper.bmp\n",
      "Saving ppt3.bmp\n",
      "Saving woman_GT.bmp\n",
      "Saving zebra.bmp\n"
     ]
    }
   ],
   "source": [
    "prepare_images(\"source_images\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baboon.bmp\n",
      "PSNR: 22.157084083442548\n",
      " MSE: 1187.1161333333334\n",
      "SSIM: 0.629277587900277\n",
      "\n",
      "baby_GT.bmp\n",
      "PSNR: 34.37180640966199\n",
      " MSE: 71.28874588012695\n",
      "SSIM: 0.9356987872724932\n",
      "\n",
      "barbara.bmp\n",
      "PSNR: 25.906629837568126\n",
      " MSE: 500.65508535879627\n",
      "SSIM: 0.8098632646406401\n",
      "\n",
      "bird_GT.bmp\n",
      "PSNR: 32.896644728720005\n",
      " MSE: 100.12375819830247\n",
      "SSIM: 0.9533644866026473\n",
      "\n",
      "butterfly_GT.bmp\n",
      "PSNR: 24.782076560337416\n",
      " MSE: 648.6254119873047\n",
      "SSIM: 0.8791344763843051\n",
      "\n",
      "coastguard.bmp\n",
      "PSNR: 27.161600663887082\n",
      " MSE: 375.00887784090907\n",
      "SSIM: 0.756950063354931\n",
      "\n",
      "comic.bmp\n",
      "PSNR: 23.799861502225532\n",
      " MSE: 813.2338836565096\n",
      "SSIM: 0.8347335416398209\n",
      "\n",
      "face.bmp\n",
      "PSNR: 30.99220650287191\n",
      " MSE: 155.23189718546524\n",
      "SSIM: 0.8008439492289884\n",
      "\n",
      "flowers.bmp\n",
      "PSNR: 27.454504805386147\n",
      " MSE: 350.55093922651935\n",
      "SSIM: 0.8697286286974628\n",
      "\n",
      "foreman.bmp\n",
      "PSNR: 30.14456532664372\n",
      " MSE: 188.6883483270202\n",
      "SSIM: 0.933268417388899\n",
      "\n",
      "head_GT.bmp\n",
      "PSNR: 31.020502848237534\n",
      " MSE: 154.2237755102041\n",
      "SSIM: 0.8011121330733371\n",
      "\n",
      "lenna.bmp\n",
      "PSNR: 31.47349297867539\n",
      " MSE: 138.94800567626953\n",
      "SSIM: 0.8460989200521499\n",
      "\n",
      "monarch.bmp\n",
      "PSNR: 30.196242365288896\n",
      " MSE: 186.45643615722656\n",
      "SSIM: 0.9439574293434104\n",
      "\n",
      "pepper.bmp\n",
      "PSNR: 29.88947161686106\n",
      " MSE: 200.1033935546875\n",
      "SSIM: 0.8357937568464359\n",
      "\n",
      "ppt3.bmp\n",
      "PSNR: 24.84926168950471\n",
      " MSE: 638.6684263912582\n",
      "SSIM: 0.9284023942315316\n",
      "\n",
      "woman_GT.bmp\n",
      "PSNR: 29.326236280817465\n",
      " MSE: 227.812729498164\n",
      "SSIM: 0.9335397280466592\n",
      "\n",
      "zebra.bmp\n",
      "PSNR: 27.909840639329513\n",
      " MSE: 315.6585459528818\n",
      "SSIM: 0.8911656209329116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the generated images using image quality metrics\n",
    "\n",
    "for file in os.listdir(\"images/\"):\n",
    "    \n",
    "    # Open the target and ref image\n",
    "    target = cv2.imread(\"images/{}\".format(file))\n",
    "    ref = cv2.imread(\"source_images/{}\".format(file))\n",
    "    \n",
    "    # calculate the scores\n",
    "    scores = compare_images(target, ref)\n",
    "    \n",
    "    # Print the file names\n",
    "    print('{}\\nPSNR: {}\\n MSE: {}\\nSSIM: {}\\n'.format(file,scores[0],scores[1],scores[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the srcnn model\n",
    "\n",
    "def model():\n",
    "    \n",
    "    # define model type\n",
    "    SRCNN = Sequential()\n",
    "    \n",
    "    # add model layers\n",
    "    SRCNN.add(Conv2D(filters=128, kernel_size = (9,9), kernel_initializer='glorot_uniform',\n",
    "                    activation='relu', padding = 'valid', use_bias=True, input_shape = (None,None,1)))\n",
    "    \n",
    "    SRCNN.add(Conv2D(filters=64, kernel_size = (3,3), kernel_initializer='glorot_uniform',\n",
    "                    activation='relu', padding = 'same', use_bias=True))\n",
    "    \n",
    "    SRCNN.add(Conv2D(filters=1, kernel_size = (5,5), kernel_initializer='glorot_uniform',\n",
    "                    activation='linear', padding = 'valid', use_bias=True, input_shape = (None,None,1)))\n",
    "    \n",
    "    # Define optimizer\n",
    "    adam = Adam(lr = 0.0003)\n",
    "    \n",
    "    # Compile model\n",
    "    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return SRCNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary images processing functions\n",
    "\n",
    "def modcrop(img, scale):\n",
    "    \n",
    "    tmpsz = img.shape\n",
    "    sz = tmpsz[0:2]\n",
    "    sz = sz - np.mod(sz, scale)\n",
    "    img = img[0:sz[0],1:sz[1]]\n",
    "    \n",
    "    return img\n",
    "\n",
    "def shave(image, border):\n",
    "    img = image[border: -border, border: -border]\n",
    "    return img\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the main prediction function\n",
    "\n",
    "def predict(image_path):\n",
    "    \n",
    "    # load the SRCNN model with weights\n",
    "    srcnn = model()\n",
    "    srcnn.load_weights('3051crop_weight_200.h5')\n",
    "    \n",
    "    # load in our degraded and reference image\n",
    "    path, file = os.path.split(image_path)\n",
    "    degraded = cv2.imread(image_path)\n",
    "    ref = cv2.imread('source_images/{}'.format(file))\n",
    "    \n",
    "    # Preprocess the images with modcrop\n",
    "    ref = modcrop(ref, 3)\n",
    "    degraded = modcrop(degraded, 3)\n",
    "    \n",
    "    # Convert the image to YCrCb (SRCNN is trained on Y channel)\n",
    "    temp = cv2.cvtColor(degraded, cv2.COLOR_BGR2YCrCb)  \n",
    "    \n",
    "    # create image size and normalize \n",
    "    Y = np.zeros((1, temp.shape[0], temp.shape[1], 1), dtype=float)\n",
    "    \n",
    "    Y[0, :, :, 0] = temp[:, :, 0].astype(float) / 255;\n",
    "    \n",
    "    # Perform super resolution with SRCNN network\n",
    "    pre = srcnn.predict(Y, batch_size = 1)\n",
    "    \n",
    "    # Post process output\n",
    "    pre *= 255\n",
    "    pre[pre[:] > 255] = 255\n",
    "    pre[pre[:] < 0] = 0\n",
    "    pre = pre.astype(np.uint8)\n",
    "    \n",
    "    # Copy Y channel back to image and convert o BGR\n",
    "    temp = shave(temp, 6)\n",
    "    temp[:, :, 0] = pre[0, : , :, 0]\n",
    "    output = cv2.cvtColor(temp, cv2.COLOR_YCrCb2BGR)\n",
    "    \n",
    "    # Remove border from ref and degraded image\n",
    "    ref = shave(ref.astype(np.uint8),6)\n",
    "    degraded = shave(ref.astype(np.uint8),6)\n",
    "    \n",
    "    # Image quality calcuations\n",
    "    scores = []\n",
    "    scores.append(compare_images(degraded, ref))\n",
    "    scores.append(compare_images(output, ref))\n",
    "    \n",
    "    # return images and scores\n",
    "    \n",
    "    return ref, degraded, output, scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (348,485,3) (336,473,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3c41f18bf69b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegraded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source_images/flowers.bmp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Print all scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Degraded images \\nPSNR: {}\\n MSE: {}\\nSSIM: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reconstructed images \\nPSNR: {}\\n MSE: {}\\nSSIM: {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-9d7f44ce8ae2>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Image quality calcuations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompare_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegraded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompare_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a45bdbca153f>\u001b[0m in \u001b[0;36mcompare_images\u001b[1;34m(target, ref)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompare_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsnr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Structural similarity index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a45bdbca153f>\u001b[0m in \u001b[0;36mpsnr\u001b[1;34m(target, ref)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mref_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mref_data\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (348,485,3) (336,473,3) "
     ]
    }
   ],
   "source": [
    "ref, degraded, output , scores = predict('source_images/flowers.bmp')\n",
    "\n",
    "# Print all scores\n",
    "print('Degraded images \\nPSNR: {}\\n MSE: {}\\nSSIM: {}\\n'.format(scores[0][0],scores[0][1],scores[0][2]))\n",
    "print('Reconstructed images \\nPSNR: {}\\n MSE: {}\\nSSIM: {}\\n'.format(scores[1][0],scores[1][1],scores[1][2]))\n",
    "\n",
    "# display images as subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize = (20,8))\n",
    "axs[0].imshow(cv2.cvColor(ref, cv2.COLOR_BGR2RGB))\n",
    "axs[0].set_title('Original')\n",
    "axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n",
    "axs[1].set_title('Degraded')\n",
    "axs[2].imshow(cv2.cvColor(output, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "#Remove the x and y tick marks\n",
    "for ax in axs:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
